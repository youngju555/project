# -*- coding: utf-8 -*-
"""
MyCobot 320 M5 (pymycobot)
[YOLO + ì¹´ë©”ë¼ë³´ì • ê¸°ë°˜ ì¢Œí‘œë³€í™˜ + ìŠ¤ë ˆë“œ ë¶„ë¦¬ + ê°ì§€ í›„ ì¹´ë©”ë¼ ìë™ì¢…ë£Œ + ì¢Œí‘œì €ì¥ v8.0]

ğŸ“Œ ì „ì²´ ìˆœì„œ
-------------------------------------------------
1ï¸âƒ£ ì¹´ë©”ë¼ ìŠ¤ë ˆë“œ: í”„ë ˆì„ ì†¡ì¶œë§Œ ìˆ˜í–‰
2ï¸âƒ£ ë©”ì¸ ë£¨í”„: ROI ë‚´ YOLO ê°ì§€ â†’ 3ì´ˆ ìœ ì§€ ì‹œ
3ï¸âƒ£ ì¢Œí‘œ ê³„ì‚°(pixel_to_robot) + JSON ì €ì¥
4ï¸âƒ£ ì¹´ë©”ë¼ ì¢…ë£Œ â†’ ë¡œë´‡ ì´ë™ (Homeâ†’Pickâ†’Placeâ†’Home)
"""

import threading
import cv2
import time
import argparse
import numpy as np
import json
import os
from ultralytics import YOLO

# ======================================================
# 0ï¸âƒ£ ë¡œë´‡ í´ë˜ìŠ¤ ë¡œë“œ
# ======================================================
try:
    from pymycobot.mycobot320 import MyCobot320 as CobotClass
except Exception:
    from pymycobot.mycobot import MyCobot as CobotClass


# ======================================================
# 1ï¸âƒ£ í¬ì¦ˆ ì •ì˜
# ======================================================
POSES = {
    "Home":  [59.8, -215.9, 354.6, -175.33, 8.65, 86.68],
    "Clear": [264.0, -1.0, 379.0, -153, 11, -106],
    "Place": [333.0, 11.0, 170.0, -175, -0.08, -89.0],
}
DEFAULT_SPEED = 20



# ======================================================
# 2ï¸âƒ£ ì¹´ë©”ë¼ ë³´ì •ê°’ ë¡œë“œ
# ======================================================
def load_camera_params(yaml_path="/home/vboxuser/robotarm/camera_info.yaml"):
    fs = cv2.FileStorage(yaml_path, cv2.FILE_STORAGE_READ)
    if not fs.isOpened():
        raise FileNotFoundError(f"âŒ '{yaml_path}' íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    camera_matrix = fs.getNode("camera_matrix").mat()
    dist_coeffs = fs.getNode("distortion_coefficients").mat()
    fs.release()
    print("ğŸ“· ì¹´ë©”ë¼ ë³´ì • íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ")
    return camera_matrix, dist_coeffs


# ======================================================
# 3ï¸âƒ£ í”½ì…€ â†’ ë¡œë´‡ ì¢Œí‘œ ë³€í™˜ (ì˜¤í”„ì…‹ í¬í•¨)
# ======================================================
def pixel_to_robot(cx, cy, distance_cm, camera_matrix, dist_coeffs):
    pts = np.array([[[cx, cy]]], dtype=np.float32)
    undistorted = cv2.undistortPoints(pts, camera_matrix, dist_coeffs, P=None)
    norm_x, norm_y = undistorted[0, 0]

    # ê¹Šì´ ê³„ì‚° (cm â†’ mm)
    scale_z = distance_cm * 10.0
    x_cam = norm_x * scale_z
    y_cam = norm_y * scale_z

    # ----------------------------------------
    # ğŸ“ ì˜¤í”„ì…‹ (í…ŒìŠ¤íŠ¸ ê¸°ì¤€)
    # ----------------------------------------
    TCP_BASE_OFFSET_X = 59.8
    TCP_BASE_OFFSET_Y = -215.9
    TCP_BASE_OFFSET_Z = 354.6
    CAMERA_TO_TCP_OFFSET_X = 75   # â† ì¹´ë©”ë¼ê°€ Xë°©í–¥ìœ¼ë¡œ 90mm ì•ì— ìˆìŒ
    CAMERA_TO_TCP_OFFSET_Y = 0.0
    CAMERA_TO_TCP_OFFSET_Z = 170.0  # â† ì‹¤ì œ ë†’ì´ ì°¨ì´ (í˜„ì¬ëŠ” ì‚¬ìš© ì•ˆ í•¨)

    # ----------------------------------------
    # ë¡œë´‡ ì¢Œí‘œ ê³„ì‚°
    # ----------------------------------------
    robot_x = TCP_BASE_OFFSET_X + CAMERA_TO_TCP_OFFSET_X + y_cam
    robot_y = TCP_BASE_OFFSET_Y + CAMERA_TO_TCP_OFFSET_Y + x_cam

    # ZëŠ” í˜„ì¬ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ê³ ì • (ì›€ì§ì´ì§€ ì•ŠìŒ)
    robot_z = TCP_BASE_OFFSET_Z   # scale_z ì ìš© ì•ˆ í•¨

    return {"x": round(robot_x, 2), "y": round(robot_y, 2), "z": round(robot_z, 2)}



# ======================================================
# 4ï¸âƒ£ YOLO ê°ì§€ í•¨ìˆ˜
# ======================================================
def detect_yolo(model, frame):
    results = model.predict(frame, imgsz=640, conf=0.6, verbose=False)
    boxes = results[0].boxes.xyxy.cpu().numpy()
    frame_vis = results[0].plot()
    detected_info = []
    FIXED_DISTANCE_CM = 30.0

    if len(boxes) > 0:
        x1, y1, x2, y2 = boxes[0]
        cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)
        w_box, h_box = (x2 - x1), (y2 - y1)
        bbox = (x1, y1, w_box, h_box)
        detected_info.append(("object", (cx, cy), FIXED_DISTANCE_CM, bbox))  # âœ… bbox í¬í•¨
    return frame_vis, detected_info


# ======================================================
# 5ï¸âƒ£ ì¹´ë©”ë¼ ìŠ¤ë ˆë“œ (í”„ë ˆì„ ì†¡ì¶œë§Œ)
# ======================================================
def camera_capture_thread(stop_event, frame_container):
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âš ï¸ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("ğŸ“· ì¹´ë©”ë¼ ìŠ¤ë ˆë“œ ì‹œì‘ (í”„ë ˆì„ ì†¡ì¶œ ì¤‘...)")
    while not stop_event.is_set():
        ret, frame = cap.read()
        if not ret:
            time.sleep(0.01)
            continue
        frame_container["frame"] = frame
    cap.release()
    print("ğŸ“· ì¹´ë©”ë¼ ìŠ¤ë ˆë“œ ì¢…ë£Œ")


# ======================================================
# 6ï¸âƒ£ ë¡œë´‡ ì´ë™ í—¬í¼
# ======================================================
def move_to(mc, name, speed=DEFAULT_SPEED):
    if name not in POSES:
        print(f"âš ï¸ Unknown pose: {name}")
        return
    target = POSES[name]
    mc.send_coords(target, speed, 0)
    time.sleep(2)
    print(f"âœ… Move â†’ {name}")


# ======================================================
# 7ï¸âƒ£ ì¢Œí‘œ JSON ì €ì¥
# ======================================================
def save_pick_coordinate(coord, filename="picking_target.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(coord, f, indent=4, ensure_ascii=False)
    print(f"ğŸ’¾ ì¢Œí‘œ ì €ì¥ ì™„ë£Œ â†’ {filename} : {coord}")


# ======================================================
# 8ï¸âƒ£ ë©”ì¸ ë£¨í”„
# ======================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=str, default="/dev/ttyACM0")
    parser.add_argument("--baud", type=int, default=115200)
    parser.add_argument("--speed", type=int, default=20)
    parser.add_argument("--model", type=str, default="/home/vboxuser/robotarm/best.pt")
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    detected_angle = None     

    # YOLO ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ§  YOLO ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model}")
    model = YOLO(args.model)
    print("âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # ì¹´ë©”ë¼ ë³´ì •ê°’ ë¡œë“œ
    camera_matrix, dist_coeffs = load_camera_params()

    # ë¡œë´‡ ì—°ê²°
    mc = None
    if not args.dry_run:
        mc = CobotClass(args.port, args.baud)
        time.sleep(0.5)
        mc.power_on()
        print("ğŸ”Œ Power ON ì™„ë£Œ")
        move_to(mc, "Home", args.speed)
        mc.set_gripper_mode(0)
        mc.set_electric_gripper(0)
        mc.set_gripper_value(0, 20, 1)  # ì—´ë¦¼
    else:
        print("ğŸŸ¡ dry-run ëª¨ë“œ (ë¡œë´‡ ë¯¸ì—°ê²°)")

    # ì¹´ë©”ë¼ ìŠ¤ë ˆë“œ ì‹œì‘
    frame_container = {"frame": None}
    stop_event = threading.Event()
    cam_thread = threading.Thread(
        target=camera_capture_thread, args=(stop_event, frame_container), daemon=True
    )
    cam_thread.start()

    print("âœ… ë©”ì¸ ë£¨í”„ ì‹œì‘ (ROI ê°ì§€ í›„ 3ì´ˆ ìœ ì§€ ì‹œ ì‹¤í–‰)")
    roi_detect_start = None
    DETECT_HOLD_TIME = 3.0
    detected_coord = None

    try:
        while not stop_event.is_set():
            frame = frame_container.get("frame")
            if frame is None:
                continue

            # ROI í‘œì‹œ
            h, w, _ = frame.shape
            roi_x1, roi_y1 = int(w * 0.3), int(h * 0.3)
            roi_x2, roi_y2 = int(w * 0.7), int(h * 0.7)
            cv2.rectangle(frame, (roi_x1, roi_y1), (roi_x2, roi_y2), (0, 255, 0), 2)
            cv2.drawMarker(frame, (w // 2, h // 2), (0, 255, 0), cv2.MARKER_CROSS, 15, 2)

            # YOLO ê°ì§€ ìˆ˜í–‰
            processed_frame, detected = detect_yolo(model, frame)
            in_roi = False
            angle = None  # ğŸ”¸ ìƒˆë¡œ ì¶”ê°€: ê°ë„ ì´ˆê¸°í™”

            # ê°ì§€ ê²°ê³¼ ìˆì„ ë•Œ
            if detected:
                _, (cx, cy), dist, bbox = detected[0]
                if roi_x1 < cx < roi_x2 and roi_y1 < cy < roi_y2:
                    in_roi = True

                    # -------------------------------
                    # ğŸ”¸ YOLO bounding box ê¸°ë°˜ ê°ë„ ê³„ì‚°
                    # -------------------------------
                    # detect_yoloê°€ bounding boxë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš° ì˜ˆì‹œ:
                    # bbox = (x, y, w_box, h_box)
                    if bbox is not None and isinstance(bbox, tuple):
                        x, y, w_box, h_box = bbox
                        # ROI ë‚´ contour ë°•ìŠ¤ ìƒì„±
                        box_points = np.array([
                            [x, y],
                            [x + w_box, y],
                            [x + w_box, y + h_box],
                            [x, y + h_box]
                        ])
                        rect = cv2.minAreaRect(box_points)
                        ((cx_rect, cy_rect), (bw, bh), angle) = rect
                        detected_angle = angle 

                        # ì‹œê°í™” (ë°•ìŠ¤ + ì¤‘ì‹¬ + ê°ë„ í‘œì‹œ)
                        box = cv2.boxPoints(rect)
                        box = np.intp(box)
                        cv2.drawContours(frame, [box], 0, (255, 255, 0), 2)
                        cv2.circle(frame, (int(cx_rect), int(cy_rect)), 5, (0, 0, 255), -1)
                        cv2.putText(frame, f"{angle:.1f} deg", (int(cx_rect) - 40, int(cy_rect) + 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            # ROI ë‚´ë¶€ì— ê°ì§€ëœ ê²½ìš° (3ì´ˆ ìœ ì§€ ì‹œ ì¢Œí‘œ í™•ì •)
            if in_roi:
                if roi_detect_start is None:
                    roi_detect_start = time.time()
                    print("ğŸ”µ ROI ê°ì§€ ì‹œì‘ (3ì´ˆ ìœ ì§€ ì‹œ ì¢Œí‘œ í™•ì •)")
                else:
                    elapsed = time.time() - roi_detect_start
                    cv2.putText(processed_frame, f"ê°ì§€ ì¤‘... {elapsed:.1f}s", (20, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
                    if elapsed >= DETECT_HOLD_TIME:
                        print("ğŸŸ¢ ê°ì§€ ìœ ì§€ 3ì´ˆ â†’ ì¢Œí‘œ ê³„ì‚° ì‹œì‘")
                        detected_coord = pixel_to_robot(cx, cy, dist, camera_matrix, dist_coeffs)
                        print(f"ğŸ¯ ë¬¼ì²´ ì¢Œí‘œ: {detected_coord}, ê°ë„: {detected_angle}")  # âœ… ì €ì¥ëœ ê°ë„ ì¶œë ¥
                            
                        # # -------------------------------
                        # # ğŸ”¸ 6ì¶• íšŒì „ ë³´ì • (angle ê¸°ì¤€)
                        # # -------------------------------
                        # if angle is not None:
                        #     try:
                        #         current_angles = mc.get_angles()
                        #         rotation_correction = angle  # í•„ìš” ì‹œ Â± ë°©í–¥ ë³´ì • í…ŒìŠ¤íŠ¸
                        #         current_angles[5] += rotation_correction
                        #         mc.send_angles(current_angles, 20)
                        #         print(f"ğŸ”„ 6ì¶• íšŒì „ ë³´ì • ì™„ë£Œ ({rotation_correction:.1f}Â°)")
                        #     except Exception as e:
                        #         print(f"âš ï¸ íšŒì „ ë³´ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        

                        # -------------------------------
                        # ğŸ”¸ ê·¸ë¦¬í¼ ë™ì‘ (ì„ íƒ ì‚¬í•­)
                        # -------------------------------
                        try:
                            mc.set_gripper_state(0, 80)   # ì™„ì „ ì—´ê¸°
                            mc.set_gripper_state(1, 80)   # ì™„ì „ ì—´ê¸°
                            print("ğŸ¤– ê·¸ë¦¬í¼ ë™ì‘ ì™„ë£Œ")
                        except Exception as e:
                            print(f"âš ï¸ ê·¸ë¦¬í¼ ë™ì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

                        # -------------------------------
                        # âœ… ê°ì§€ ì™„ë£Œ í›„ ì¹´ë©”ë¼ ì¢…ë£Œ
                        # -------------------------------
                        stop_event.set()
                        cam_thread.join()
                        cv2.destroyAllWindows()
                        print("ğŸ“· ì¹´ë©”ë¼ ì¢…ë£Œ ì™„ë£Œ")
                        break
            else:
                roi_detect_start = None


            cv2.imshow("Camera View", processed_frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                stop_event.set()
                break

    finally:
        stop_event.set()
        cam_thread.join()
        cv2.destroyAllWindows()

    # ==================================================
    # âœ… ê°ì§€ëœ ì¢Œí‘œê°€ ìˆìœ¼ë©´ ë¡œë´‡ ì´ë™
    # ==================================================
    if detected_coord:
        print("ğŸ¤– ë¡œë´‡ ì´ë™ ì‹œì‘...")
        mc.set_gripper_mode(0)
        mc.set_electric_gripper(0)
        if not args.dry_run and mc:
            base_r = -175.33
            base_p = 8.65
            base_y = 86.68
            yaw_offset = (detected_angle if detected_angle is not None else 0.0) * 0.35
            wrist_yaw = base_y + yaw_offset   # ğŸ“Œ YOLO ê°ë„ ë°˜ì˜
            print(f"ğŸ§­ Wrist íšŒì „ ì ìš©: base_y={base_y:.1f}, offset={yaw_offset:.1f} â†’ ìµœì¢…={wrist_yaw:.1f}")


            mc.set_gripper_value(50, 20, 1)  # ì—´ë¦¼

            # ìœ„ì—ì„œ ì ‘ê·¼
            mc.send_coords(
                [detected_coord["x"], detected_coord["y"], 300.0,
                base_r, base_p, wrist_yaw],
                25, 0
            )
            time.sleep(3)

            # ë‚´ë ¤ê°€ì„œ ì§‘ê¸°
            mc.send_coords(
                [detected_coord["x"], detected_coord["y"], 260.0+40,
                base_r, base_p, wrist_yaw],
                15, 0
            )
            time.sleep(2)

            mc.set_gripper_value(8, 20, 1)  # ë‹«í˜

            # ìœ„ë¡œ ë¹¼ê¸°
            mc.send_coords(
                [detected_coord["x"], detected_coord["y"], 260.0+100,
                base_r, base_p, wrist_yaw],
                15, 0
            )
            #ë©ˆì¶¤
            # exit()
            time.sleep(1.5)
            move_to(mc, "Clear", args.speed)
            move_to(mc, "Place", args.speed)
            mc.set_gripper_state(0, 80)
            move_to(mc, "Home", args.speed)
        else:
            print(f"ğŸŸ¢ [dry-run] ì¢Œí‘œ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {detected_coord}")

    if mc:
        mc.power_off()
    print("ğŸ”’ ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    main()
